/**
 * ç¤ºä¾‹ 26: æ€§èƒ½ä¸æˆæœ¬ä¼˜åŒ– (Performance & Cost Optimization)
 * 
 * æ ¸å¿ƒä»·å€¼ï¼š
 * 1. Prompt Caching (æç¤ºè¯ç¼“å­˜): é’ˆå¯¹æ”¯æŒç¼“å­˜çš„æ¨¡å‹ï¼ˆå¦‚ Anthropic, DeepSeekï¼‰ï¼Œé€šè¿‡ç»“æ„åŒ– Prompt æœ€å¤§åŒ–ç¼“å­˜å‘½ä¸­ï¼Œé™ä½ 90% å»¶è¿Ÿã€‚
 * 2. Fine-grained Token Control (ç²¾ç»†åŒ– Token æ§åˆ¶): åœ¨å‘é€è¯·æ±‚å‰ç²¾ç¡®è®¡ç®— Tokenï¼Œå¹¶æ ¹æ®é¢„ç®—åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
 */

const { generateText } = require('ai');
const { cloud, local } = require('../../lib/ai-providers');
require('dotenv').config();

/**
 * æ¨¡æ‹Ÿ countTokens å·¥å…·
 * æ³¨æ„ï¼šåœ¨ Vercel AI SDK çš„æœ€æ–°ç‰ˆæœ¬æˆ–ç‰¹å®š Provider ä¸­ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ countTokens å‡½æ•°ã€‚
 * è¿™é‡Œæˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„ä¼°ç®—å™¨ç”¨äºæ¼”ç¤ºé€»è¾‘ã€‚
 */
async function countTokensMock({ model, messages }) {
  const text = messages.map(m => typeof m.content === 'string' ? m.content : JSON.stringify(m.content)).join('');
  // ç²—ç•¥ä¼°ç®—ï¼šè¿™é‡Œæˆ‘ä»¬æ•…æ„ä¼°ç®—å¾—ç¨å¾®å¤§ä¸€ç‚¹ï¼Œä»¥ç¡®ä¿ä¸ä¼šè¶…å‡ºçœŸå®é¢„ç®—
  return Math.ceil(text.length / 1.1);
}

/**
 * åœºæ™¯ 1: Prompt Caching é€‚é…
 * ç­–ç•¥ï¼šå°†â€œé™æ€â€ä¸”â€œæ˜‚è´µâ€çš„å†…å®¹ï¼ˆå¦‚çŸ¥è¯†åº“ã€ç³»ç»ŸæŒ‡ä»¤ï¼‰æ”¾åœ¨ Prompt çš„æœ€å‰é¢ã€‚
 * 
 * DeepSeek ç¼“å­˜æœºåˆ¶è¯´æ˜ï¼š
 * - DeepSeek è‡ªåŠ¨ç¼“å­˜å·²å¤„ç†è¿‡çš„ Prompt å‰ç¼€ã€‚
 * - ç¼“å­˜ä»¥ 64 Tokens ä¸ºä¸€ä¸ªåŒºå—è¿›è¡ŒåŒ¹é…ã€‚
 * - ä¸ºäº†æœ€å¤§åŒ–å‘½ä¸­ç‡ï¼Œåº”ç¡®ä¿æ¶ˆæ¯åˆ—è¡¨çš„å¼€å¤´éƒ¨åˆ†ï¼ˆSystem Prompt + é™æ€å‚è€ƒèµ„æ–™ï¼‰ä¿æŒä¸å˜ã€‚
 */
async function promptCachingDemo() {
  console.log('\n--- [åœºæ™¯ 1] Prompt Caching é€‚é…æ¼”ç¤º ---');

  // æ¨¡æ‹Ÿä¸€ä¸ªå·¨å¤§çš„é™æ€çŸ¥è¯†åº“ (Context)
  const hugeKnowledgeBase = `
    è¿™é‡Œæ˜¯å…¬å¸çš„æ ¸å¿ƒä¸šåŠ¡æ–‡æ¡£... (çœç•¥ 5000 å­—)
    1. æŠ¥é”€æµç¨‹ï¼šæäº¤ç”³è¯· -> ç»ç†å®¡æ‰¹ -> è´¢åŠ¡æ‹¨æ¬¾ã€‚
    2. å…¥èŒæŒ‡å—ï¼šé¢†å–ç”µè„‘ -> è®¾ç½®é‚®ç®± -> å‚åŠ åŸ¹è®­ã€‚
    ... æ›´å¤šé™æ€å†…å®¹ ...
  `;

  const systemPrompt = "ä½ æ˜¯ä¸€ä¸ªä¼ä¸šåŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„çŸ¥è¯†åº“å›ç­”é—®é¢˜ã€‚";

  // æ¨èçš„ç»“æ„ï¼šSystem + Knowledge Base (Static) + Messages (Dynamic)
  const messages = [
    { role: 'system', content: systemPrompt },
    { role: 'user', content: `å‚è€ƒçŸ¥è¯†åº“ï¼š\n${hugeKnowledgeBase}` },
    { role: 'user', content: "å¸®æˆ‘æŸ¥ä¸€ä¸‹æŠ¥é”€æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ" }
  ];

  console.log('ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼šå°†å·¨å¤§çš„çŸ¥è¯†åº“ä½œä¸ºç¬¬ä¸€æ¡ User æ¶ˆæ¯æˆ– System æ¶ˆæ¯çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¿æŒé¡ºåºä¸å˜ã€‚');
  console.log('è¿™æ ·åç»­æ‰€æœ‰åŸºäºæ­¤çŸ¥è¯†åº“çš„æé—®ï¼Œå‰å‡ åƒä¸ª Tokens éƒ½ä¼šå‘½ä¸­ DeepSeek çš„ç¼“å­˜ï¼Œä»…éœ€æ”¯ä»˜æä½çš„ç¼“å­˜è´¹ç”¨ï¼Œä¸”å“åº”å‡ ä¹ç¬æ—¶ã€‚');
  console.log('\n--- ä¾›åº”å•†ç‰¹å®šè¯´æ˜ ---');
  console.log('- Azure OpenAI: è‡ªåŠ¨ç¼“å­˜ã€‚éœ€ç¡®ä¿å‰ç¼€è¶…è¿‡ 1024 Tokens ä»¥è§¦å‘æ”¶ç›Šã€‚');
  console.log('- Google Gemini: æ˜¾å¼ç¼“å­˜ã€‚é€šè¿‡ Context Caching API åˆ›å»ºç¼“å­˜ IDï¼Œé€‚åˆ 1M+ Tokens çš„è¶…é•¿ä¸Šä¸‹æ–‡ã€‚');

  if (process.env.DEEPSEEK_API_KEY) {
    try {
      const result = await generateText({
        model: cloud.deepseek,
        messages: messages,
      });
      console.log('âœ… å“åº”æˆåŠŸ (å¦‚æœå¤šæ¬¡è¿è¡Œï¼Œä½ ä¼šå‘ç°é¦–å­—å»¶è¿Ÿæä½)');
      console.log(`[Usage]: Input ${result.usage.inputTokens}, Output ${result.usage.outputTokens}`);
    } catch (e) {
      console.log('è·³è¿‡å®é™…è°ƒç”¨ (API Key å¯èƒ½æœªé…ç½®)');
    }
  } else {
    console.log('è·³è¿‡å®é™…è°ƒç”¨ï¼šæœªå‘ç° DEEPSEEK_API_KEY');
  }
}

/**
 * åœºæ™¯ 2: Fine-grained Token Control (ç²¾ç»†åŒ– Token æ§åˆ¶)
 * ç­–ç•¥ï¼šåˆ©ç”¨ countTokens åœ¨å‘é€å‰è¿›è¡Œé¢„ç®—è¯„ä¼°ï¼ŒåŠ¨æ€æˆªæ–­å†å²è®°å½•ã€‚
 */
async function tokenControlDemo() {
  console.log('\n--- [åœºæ™¯ 2] Fine-grained Token Control æ¼”ç¤º ---');

  const MAX_TOKEN_BUDGET = 50; // è°ƒä½é¢„ç®—ä»¥æ¼”ç¤ºæˆªæ–­é€»è¾‘
  const model = local.chat; // ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæ¼”ç¤º

  const history = [
    { role: 'user', content: 'ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£å…³äº AI å‘å±•çš„å†å²ã€‚' },
    { role: 'assistant', content: 'AI çš„å‘å±•ç»å†äº†å‡ ä¸ªé˜¶æ®µï¼Œä» 1956 å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®å¼€å§‹...' },
    { role: 'user', content: 'é‚£ä½ èƒ½è¯¦ç»†è¯´è¯´ç¬¬äºŒæ¬¡ AI æµªæ½®å—ï¼Ÿ' },
    { role: 'assistant', content: 'ç¬¬äºŒæ¬¡æµªæ½®ä¸»è¦é›†ä¸­åœ¨ä¸“å®¶ç³»ç»Ÿå’ŒçŸ¥è¯†åº“çš„åº”ç”¨...' },
  ];

  const currentQuery = 'ç°åœ¨æˆ‘ä»¬å¤„äºå“ªä¸ªé˜¶æ®µï¼Ÿ';

  /**
   * åŠ¨æ€æˆªæ–­é€»è¾‘
   */
  async function getMessagesWithinBudget(history, query, budget) {
    let selectedHistory = [...history];
    
    while (selectedHistory.length > 0) {
      const messages = [...selectedHistory, { role: 'user', content: query }];
      
      // æ ¸å¿ƒå·¥å…·ï¼šcountTokens (è¿™é‡Œä½¿ç”¨æˆ‘ä»¬çš„æ¨¡æ‹Ÿå‡½æ•°)
      const tokens = await countTokensMock({
        model: model,
        messages: messages
      });

      console.log(`å½“å‰å°è¯•çš„å†å²é•¿åº¦: ${selectedHistory.length} æ¡, é¢„ä¼° Tokens: ${tokens}`);

      if (tokens <= budget) {
        return messages;
      }

      // å¦‚æœè¶…å‡ºé¢„ç®—ï¼Œç§»é™¤æœ€æ—§çš„ä¸€è½®å¯¹è¯ (ä¸€é—®ä¸€ç­”)
      selectedHistory.splice(0, 1);
    }

    return [{ role: 'user', content: query }];
  }

  console.log(`ç›®æ ‡é¢„ç®—: ${MAX_TOKEN_BUDGET} Tokens`);
  const finalMessages = await getMessagesWithinBudget(history, currentQuery, MAX_TOKEN_BUDGET);

  console.log(`\næœ€ç»ˆå‘é€çš„æ¶ˆæ¯æ¡æ•°: ${finalMessages.length}`);
  console.log('--- æœ€ç»ˆå‘é€å†…å®¹ ---');
  finalMessages.forEach(m => console.log(`[${m.role}]: ${m.content.slice(0, 30)}...`));

  const result = await generateText({
    model: model,
    messages: finalMessages,
  });

  console.log(`\nâœ… å®é™…æ¶ˆè€—: ${result.usage.inputTokens} Input Tokens (å®Œå…¨åœ¨ ${MAX_TOKEN_BUDGET} é¢„ç®—å†…)`);
}

async function main() {
  console.log('=== æ€§èƒ½ä¸æˆæœ¬ä¼˜åŒ–æŠ€æœ¯æ¼”ç¤º ===');
  
  await promptCachingDemo();
  await tokenControlDemo();
}

main().catch(console.error);
